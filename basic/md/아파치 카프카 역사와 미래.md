
- [아파치 카프카 역사와 미래](#아파치-카프카-역사와-미래)
  - [아파치 카프카의 탄생](#아파치-카프카의-탄생)
  - [카프카가 데이터 파이프라인으로 적합한 4가지 이유](#카프카가-데이터-파이프라인으로-적합한-4가지-이유)
    - [1. 높은 처리량](#1-높은-처리량)
    - [2. 확장성](#2-확장성)
    - [3. 영속성](#3-영속성)
    - [4. 고가용성](#4-고가용성)
  - [빅데이터 아키텍처의 종류와 카프카의 미래](#빅데이터-아키텍처의-종류와-카프카의-미래)
    - [초기 빅데이터 플랫폼](#초기-빅데이터-플랫폼)
    - [람다 아키텍처](#람다-아키텍처)
    - [카파 아키텍처](#카파-아키텍처)
    - [스트리밍 데이터 레이크](#스트리밍-데이터-레이크)
  - [퀴즈](#퀴즈)

---

# 아파치 카프카 역사와 미래

## 아파치 카프카의 탄생

- 링크드인에서는 파편화된 데이터 수집 및 분배 아키텍처를 운영하는데 큰 어려움을 겪었다.
- 데이터를 생성하고 적재하기 위해서 데이터를 생성하는 소스 어플리케이션과, 데이터가 최종 적재되는 타깃 애플리케이션을 연결해야 한다.
- 초기 운영 시 단방향 통신을 통해 애플리케이션에서 타깃 애플리케이션으로 연동하는 소스코드를 작성했으나 이는 아키텍처가 복잡하지 않아서 운영하는데 어려움은 없었다.
- 하지만, 시간이 지날 수록 아키텍처가 거대해졌고, 소스 애플리케이션과 타깃 애플리케이션의 개수가 점점 많아지며, 데이터를 전송하는 라인이 기하급수적으로 복잡해지기 시작했다.

![](/images/2022-06-04-04-33-16.png)

이를 해결하기 위해 다양한 메시징 플랫폼과 ETL 툴을 적용하여 아키텍처를 변경하려고 노력했으나 파편화된 데이터 파이프라인의 복잡도를 낮춰주는 아키텍처가 되지는 못했다.

![](/images/2022-06-04-04-33-30.png)

-> 결국 링크드인의 데이터팀은 신규 시스템을 개발했고, 그 결과물이 바로 아파치 카프카이다.
링크드인의 내부 데이터 흐름을 개선하기 위해 개발한 카프카는 각각의 애플리케이션끼리 연결하여 데이터를 처리하는 것이 아니라, 한 곳에 모아 처리할 수 있도록 **중앙집중화** 하였다.


> 메세지 큐 구조를 그대로 살린 카프카 내부 구조

![](/images/2022-06-04-04-33-46.png)

- 기존 1:1 매칭으로 개발/운영하던 데이터 파이프라인은 **커플링**으로 인해 한쪽의 이슈가 다른 한쪽의 애플리케이션에 영향을 미쳤으나, 카프카는 이러한 의존도를 없앴다.
- 이제 소스 애플리케이션에서 생성되는 데이터는 어느 타깃 애플리케이션으로 보낼 것인지 고민하지 않고 카프카로 넣으면 된다!
- 카프카 내부에 데이터가 저장되는 파티션의 동작은 FIFO 방식의 큐 자료구조와 유사하다.
- 큐에 데이터를 보내는 것이 프로듀서이고, 큐에서 데이터를 가져가는 것이 컨슈머이다.


## 카프카가 데이터 파이프라인으로 적합한 4가지 이유

### 1. 높은 처리량

- 카프카는 프로듀서가 브로커로 데이터를 보낼 때와 컨슈머가 브로커로부터 데이터를 받을 때 **모두 묶어서 전송**한다.
- 많은 양의 데이터를 송수신할 떄 맺어지는 네트워크 비용은 무시할 수 없는 규모가 된다.
- 동일한 양의 데이터를 보낼 때 네트워크 통신 횟수를 최소한으로 줄인다면 동일 시간 내에 더 많은 데이터를 전송할 수 있다.
- 많은 양의 데이터를 묶음 단위로 처리하는 배치로 빠르게 처리할 수 있기 때문에 대용량의 실시간 로그 데이터를 처리하는 데 적합하다.
- 또한 **파티션 단위**를 통해 동일 목적의 데이터를 여러 파티션에 분배하고 데이터를 병렬처리할 수 있다.
- 파티션 개수만큼 컨슈머를 늘려서 동일 시간당 데이터 처리량을 늘리는 것이다.**(스케일 아웃)**
  - 파티션 & 컨슈머를 리니어하게 늘리면서 처리량을 늘릴 수 있다.

### 2. 확장성

- 데이터 파이프라인에서 데이터를 모을 때 데이터가 얼마나 들어올 지 예측하기가 어렵다.
- 하루에 1,000건 가량 들어오는 로그 데이터라도 예상치 못한 특정 이벤트로 100만건 이상 데이터가 들어오는 경우도 있다.
- 카프카는 이러한 가변적인 환경에서 안정적으로 확장 가능하도록 설계되었다.
- 데이터가 적을 때는 카프카 클러스터의 브로커를 최소한의 개수로 운영하다가, 데이터가 많아지면 클러스터의 브로커 개수를 자연스럽게 늘려 스케일 아웃(scale-out) 할 수 있다.
- 반대로 데이터 개수가 적어지고, 추가 서버들이 더이상 필요 없어지만 브로커 개수를 줄여 스케일 인(scale-in) 할 수 있다.
- 카프카의 스케일 아웃, 스케일 인 과정은 클러스터의 무중단 운영을 지원하므로 365일 24시간 데이터를 처리해야 하는 커머스, 은행 같은 비즈니스 모델에서도 안정적인 운영이 가능해진다.

### 3. 영속성

- 영속성이란 데이터를 생성한 프로그램이 **종료되더라도 사라지지 않는 데이터의 특성**을 말한다.
- 카프카는 다른 메시징 플랫폼과 다르게 전송받은 데이터를 **메모리에 저장하지 않고, 파일 시스템에 저장한다.**
- 파일 시스템에 데이터를 적재하고 사용하는 것은 보편적으로 느리다고 생각하겠지만, 카프카는 운영체제 레벨에서 파일 시스템을 최대한 활용하는 방법으로 적용하였다.
- 운영체제에서는 파일 I/O 성능 향상을 위해 페이지 캐시(page cache) 영역을 메모리에 따로 생성하여 사용한다.
- **페이지 캐시 메모리 영역을 사용하여 한 번 읽은 파일 내용은 메모리에 저장시켰다가 다시 사용하는 방식이기 때문에 카프카가 파일 시스템에 저장하고 데이터를 저장, 전송하더라도 처리량이 높은 것이다.**
- 디스크 기반의 파일 시스템을 활용한 덕분에 브로커 애플리케이션이 장애 발생으로 인해 급작스럽게 종료되더라도 프로세스를 재시작하여 안전하게 데이터를 다시 처리할 수 있다.

### 4. 고가용성

- **3개 이상의 서버들**로 운영되는 카프카 클러스터는 일부 서버에 장애가 발생하더라도, 무중단으로 안전하고 지속적으로 데이터를 처리할 수 있다.
- 클러스터로 이루어진 카프카는 데이터의 복제(replication)를 통해 고가용성의 특징을 지니게 되었다.
- 프로듀서로 전송받은 데이터를 여러 브로커 중 1대의 브로커에만 저장하는 것이 아니라, 또 다른 브로커에도 저장하는 것이다.
- 한 브로커에 장애가 발생하더라도, 복제된 데이터가 나머지 브로커에 저장되어 있으므로 저장된 데이터를 기준으로 지속적으로 데이터 처리가 가능한 것이다.
- 이에 더하여 서버를 직접 운영하는 온프레미스(on-premise) 환경의 서버 랙 또는 퍼블릭 클라우드(public cloud)의 리전 단위 장애에도 데이터를 안전하게 복제할 수 있는 브로커 옵션들이 준비되어 있다.



## 빅데이터 아키텍처의 종류와 카프카의 미래

### 초기 빅데이터 플랫폼

![](/images/2022-06-06-04-14-29.png)

- 초기 빅데이터 플랫폼은 엔드 투 엔드로 각 서비스 애플리케이션으로부터 데이터를 배치로 모았음
- 데이터를 배치로 모으는 구조는 유연하지 못했고, 실시간으로 생성되는 데이터들에 대한 인사이트를 서비스 애플리케이션에 빠르게 전달하지 못하는 단점이 있음
- 원천 데이터로부터 파생된 데이터의 히스토리를 파악하기가 어려웠고, 계속되는 데이터의 가공으로 인해 데이터가 파편화되면서 데이터 거버넌스(data governance: 데이터 표준 및 정책)를 지키기가 어려웠다.

이를 극복하기 위해 만들어진 것이 바로 **람다 아키텍처**이다.

### 람다 아키텍처

![](/images/2022-06-06-04-13-53.png)

- 람다 아키텍처는 3가지 레이어로 나뉜다.
  - **배치 레이어**: 배치 데이터를 모아서 특정 시간, 타이밍마다 일괄 처리
  - **서빙 레이어**: 가공된 데이터를 데이터 사용자, 서비스 애플리케이션이 사용할 수 있도록 데이터가 저장된 공간
  - **스피드 레이어**: 서비스에서 생성되는 원천 데이터를 실시간으로 분석하는 용도로 사용

> **람다 아키텍처의 한계**

- 데이터를 배치 처리하는 배치 레이어와, 실시간 처리하는 스피드 레이어로 레이어를 분리한 람다 아키텍처.
- 레이어가 2개로 나뉘기 때문에 생기는 단점이 있었다.
  - 데이터를 분석, 처리하는데 필요한 로직이 2벌로 각각 레이어에 따로 존재해야 한다는 점
  - 배치 데이터와 실시간 데이터를 융합하여 처리할 때는 다소 유연하지 못한 파이프라인을 생성해야 한다는 점
- 1개의 로직을 추상화하여 배치 레이어와 스피드 레이어에 적용하는 형태를 고안한 서밍버드가 있었지만, 완벽히 해결되는 것은 아니었다.

이걸 해결하려고 나온 것이 **카파 아키텍처**이다.

### 카파 아키텍처

![](/images/2022-06-06-04-16-57.png)

- 람다 아키텍처의 단점을 해소하기 위해 제이 크랩스(Jay Kreps: 카프카를 최초로 고안한 개발자, 전 링크드인 팀장, 현 컨플루언트 CEO)는 카파 아키텍처를 제안했다.
- 람다 아키텍처에서 단점으로 부각되었던 `로직의 파편화, 디버깅, 배포, 운영 분리에 대한 이슈`를 제거하기 위해 **배치 레이어를 제거한 카파 아키텍처는 스피드 레이어에서 데이터를 모두 처리**할 수 있었으므로 엔지니어들은 더욱 효율적으로 개발과 운영에 임할 수 있게 되었다.

이걸 어떻게 달성할 수 있었을까? (배치 데이터와 스트림 데이터를 모두 스피드 레이어로 모으는 걸)

![](/images/2022-06-06-04-18-11.png)

- 로그는 배치 데이터를 스트림으로 표현하기에 적합하다.
- 일반적으로 데이터 플랫폼에서 배치 데이터를 표현할 때는 각 시점(시간별, 일자별 등)의 전체 데이터를 백업한 스냅샷 데이터를 뜻했다.
- 그러나 **배치 데이터를 로그로 표현할 땐 각 시점의 배치 데이터의 변환 기록(change log)을 시간 순서대로 기록함으로써 각 시점의 모든 스냅샷 데이터를 저장하지 않고도 배치 데이터를 표현할 수 있게 되었다.**
- 그래서 항상 타임스탬프가 필요합니다. 카프카에서는 특정 일시~특정 일시까지 기간에 대해 배치로 데이터를 보내는 것입니다.


> 배치 데이터와 스트림 데이터

| 배치 데이터                              | 스트림 데이터                                    |
| ---------------------------------------- | ------------------------------------------------ |
| 한정된(bounded) 데이터 처리              | 무한(unbounded) 데이터 처리                      |
| 대규모 배치 데이터를 위한 분산 처리 수행 | 지속적으로 들어오는 데이터를 위한 분산 처리 수행 |
| 분, 시간, 일 단위 처리를 위한 지연 발생  | 분 단위 이하 지연 발생                           |
| 복잡한 키 조인 수행                      | 단순한 키 조인 수행                              |


배치 데이터를 처리하는 방법으로는 하둡을 예로 들 수 있다.

![](/images/2022-06-06-04-22-52.png)


스트림 데이터를 배치로 사용하는 방법으로 카프카에서 예시를 들어보면 다음과 같다.

![](/images/2022-06-06-04-23-56.png)

- **스트림 데이터를 배치 데이터로 사용하는 방법은 로그에 시간을 남기는 것**이다.
- 로그에 남겨진 시간을 기준으로 데이터를 처리하면, 스트림으로 적재된 데이터도 배치로 처리할 수 있게 된다.
- 2021년 신입생 목록을 배치 데이터로 가져오기 위해서 스트림 데이터로 적재된 1월 1일부터 12월 31일까지의 데이터를 구체화된 뷰로 가져온다면, 배치로 처리할 수 있게 된다.
- 카프카는 로그에 시간(timestamp)을 남기기 때문에 이런 방식의 처리가 가능하다.


### 스트리밍 데이터 레이크

![](/images/2022-06-06-04-25-45.png)

- 2020년 카프카 서밋에서 제이 크랩스는 카파 아키텍처에서 서빙 레이어를 제거한 아키텍처인 스프리밍 데이터 레이크(streaming data lake)를 제안했다.
- 카파 아키텍처를 살펴보면, 데이터를 사용하는 고객을 위해 스트림 데이터를 서빙레이어에 저장하는 것을 알 수 있다.
- 스피드 레이어로 사용되는 카프카에 분석과 프로세싱을 완료한 거대한 용량의 데이터를 오랜 기간을 저장하고 사용할 수 있다면 서빙 레이어는 제거되어도 된다.
- 오히려 서빙 레이어와 스피드 레이어가 이중으로 관리되는 운영 리소스를 줄일 수 있는 것이다.
- 카프카 팀은 현재 아래와 같이 자주 사용하는 것/자주 사용하지 않는 것을 구분하는 티어드 스토리지 방식을 고안하고 있다.

![](/images/2022-06-06-04-27-19.png)

- 아직은 카프카를 스트리밍 데이터 레이크로 사용하기 위해 개선해야 하는 부분이 있다.
- 우선 자주 접근하지 않는 데이터를 굳이 비싼 자원(브로커의 메모리, 디스크)에 유지할 필요가 없다.
- 카프카 클러스터에서 자주 접근하지 않는 데이터는 오브젝트 스토리지와 같이 저렴하면서도 안전한 저장소에 옮겨 저장하고 자주 사용하는 데이터만 브로커에서 사용하는 구분 작업이 필요하다.


## 퀴즈

1) 카프카는 메모리에 데이터를 저장하기 때문에 장애가 났을 경우 데이터가 유실 된다.

2) 카파 아키텍처는 람다 아키텍처를 개선한 데이터 아키텍처이다.

3) 서빙 레이어는 데이터를 프로세싱하는데 사용되는 곳이다.


> 정답

1) X : 카프카는 메모리에 데이터를 저장하는 것이 아니라, 카프카 브로커 프로세스에 내부적으로 파일 시스템에 데이터를 저장(.log 단위의 세그먼트)하기 때문에 장애가 났을 때 데이터가 유실되지 않도록 하는 것이 장점이라고 했었음
2) O: 람다 아키텍처에는 배치 레이어, 스피드 레이어, 서빙 레이어가 있었는데, 배치레이어와 스피드 레이어가 같은 기능을 하는데 디버깅, 운영을 각각 따로 해야 하다보니까 배치 레이어를 없앤 것이 바로 카파 아키텍처이다.
3) X: 스피드 레이어에서 데이터를 처리한 뒤에, 데이터를 서빙하는 용도로 쓰는 것이 서빙 레이어이다. 사용자나 클라이언트가 이 데이터를 가져다가 쓰는 곳이다. 람다 아키텍처에서는 배치 레이어나 스피드 레이어, 카파 아키텍처에서는 스피드 레이어에서 데이터 프로세싱을 하는 레이어라고 할 수 있다.